# -*- coding: utf-8 -*-
"""Predicting Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P7U87-ZRa7RKWgEq2PFbAErJVLC_6b5U

# **Our Classifier will predict the label based on the given document**
"""

!wget http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip

!unzip bbc-fulltext.zip -d bbc

"""# Step 1 - Preparing Data."""

import os
import pandas as pd

# define data frame
df = pd.DataFrame()
mapping = {}

# file path to the classes.
source_path = 'bbc/bbc/'

dirs = os.listdir(source_path)

for files in sorted(dirs):
  if os.path.isdir(source_path+files):
    mapping[files] = sorted(os.listdir(source_path+files))[300:310] # For prediction ill pass on next 10 file in order for the model to make prediction.

mapping

mapping.keys()

"""### Read file and append it to a list"""

label = []
data = []

for key, values in mapping.items():
  print(key,values)
  for text_file in values:
    label.append(key)
    # read the content and remove the new line characters and stuff.
    data.append(open(source_path+key+"/"+text_file,encoding='cp1252').read().replace('\n'," "))

df['data'] = data

df.head(10)

# reshuffle the data to create a better data set
df = df.sample(frac=1).reset_index(drop=True)

"""## Saving it into pdf"""

df.to_csv('test.csv',index = False, header=False)

colname = ['Data']
df = pd.read_csv('test.csv',header = None, names = colname)

df.head(10)

df.shape # 10 document from 5 file

"""# Step 2: Creating Truth File.
Needed for validation
"""

df['label'] = label
df['data'] = data

df.to_csv('truth_test.csv',index = False, header=False)

colnames=['Label','Data'] 
df = pd.read_csv('truth_test.csv',header = None, names = colnames)

# reshuffle the data to create a better data set
df = df.sample(frac=1).reset_index(drop=True)

df.head(10)

"""# Step 2: Upload to s3 bucket"""

# pip install boto3

import boto3

session = boto3.Session(
    aws_access_key_id='ACCESS KEY',
    aws_secret_access_key='Secret Key',
)

s3 = session.resource('s3')
try:
  response = s3.meta.client.upload_file(Filename='test.csv', Bucket='lateetuds3east', Key ='test.csv')
  print("File Uploaded to s3 bucket")
except:
  print('Failed to upload')

"""# Step 3: Create Job in AWS Comprehend"""

from enum import Enum
import logging
from pprint import pprint
import boto3
from botocore.exceptions import ClientError
logger = logging.getLogger(__name__)

class JobInputFormat(Enum):
    per_file = 'ONE_DOC_PER_FILE'
    per_line = 'ONE_DOC_PER_LINE'

class ComprehendClassifier:
    """Encapsulates an Amazon Comprehend custom classifier."""
    def __init__(self, comprehend_client):
        """
        :param comprehend_client: A Boto3 Comprehend client.
        """
        self.comprehend_client = comprehend_client
        self.classifier_arn = None
      
    def describe(self, classifier_arn=None):
        """
        Gets metadata about a custom classifier, including its current status.

        :param classifier_arn: The ARN of the classifier to look up.
        :return: Metadata about the classifier.
        """
        if classifier_arn is not None:
            self.classifier_arn = classifier_arn
        try:
            response = self.comprehend_client.describe_document_classifier(
                DocumentClassifierArn=self.classifier_arn)
            classifier = response['DocumentClassifierProperties']
            logger.info("Got classifier %s.", self.classifier_arn)
        except ClientError:
            logger.exception("Couldn't get classifier %s.", self.classifier_arn)
            raise
        else:
            return classifier


    def start_job(
            self, job_name, input_bucket, input_key, input_format, output_bucket,
            output_key, data_access_role_arn):
        """
        Starts a classification job. The classifier must be trained or the job
        will fail. Input is read from the specified Amazon S3 input bucket and
        written to the specified output bucket. Output data is stored in a tar
        archive compressed in gzip format. The job runs asynchronously, so you can
        call `describe_document_classification_job` to get job status until it
        returns a status of SUCCEEDED.

        :param job_name: The name of the job.
        :param input_bucket: The Amazon S3 bucket that contains input data.
        :param input_key: The prefix used to find input data in the input
                          bucket. If multiple objects have the same prefix, all
                          of them are used.
        :param input_format: The format of the input data, either one document per
                             file or one document per line.
        :param output_bucket: The Amazon S3 bucket where output data is written.
        :param output_key: The prefix prepended to the output data.
        :param data_access_role_arn: The Amazon Resource Name (ARN) of a role that
                                     grants Comprehend permission to read from the
                                     input bucket and write to the output bucket.
        :return: Information about the job, including the job ID.
        """
        try:
            response = self.comprehend_client.start_document_classification_job(
                DocumentClassifierArn=self.classifier_arn,
                JobName=job_name,
                InputDataConfig={
                    'S3Uri': f's3://{input_bucket}/{input_key}',
                    'InputFormat': input_format.value},
                OutputDataConfig={'S3Uri': f's3://{output_bucket}/{output_key}'},
                DataAccessRoleArn=data_access_role_arn)
            logger.info(
                "Document classification job %s is %s.", job_name,
                response['JobStatus'])
        except ClientError:
            logger.exception("Couldn't start classification job %s.", job_name)
            raise
        else:
            return response

    def describe_job(self, job_id):
        """
        Gets metadata about a classification job.

        :param job_id: The ID of the job to look up.
        :return: Metadata about the job.
        """
        try:
            response = self.comprehend_client.describe_document_classification_job(
                JobId=job_id)
            job = response['DocumentClassificationJobProperties']
            logger.info("Got classification job %s.", job['JobName'])
        except ClientError:
            logger.exception("Couldn't get classification job %s.", job_id)
            raise
        else:
            return job

def usage_demo():
    print('-' * 88)
    print("Welcome to the Amazon Comprehend detection demo!")
    print('-' * 88)

    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    comp_detect = ComprehendClassifier(boto3.client('comprehend', region_name='us-east-1',
                                                    aws_access_key_id='Access Key',
                                                    aws_secret_access_key='Secret Key'))

    # todo: describe classifier

    print("Classifier Description.")
    classifier_arn = 'ARN'
    desc_classifier = comp_detect.describe(classifier_arn)

    pprint(desc_classifier)

    
    print('-' * 88)

    # todo: Start classification job

    print("Classifier Job Started.")

    job_name = 'Prediction01'

    input_bucket = 'lateetuds3east'

    input_key = 'test.csv'

    input_format = JobInputFormat.per_line

    output_bucket = 'lateetuds3east'

    output_key = 'predicted.csv'

    data_access_role_arn = 'Data Access Arn'




    start_classification_job = comp_detect.start_job(
            job_name,input_bucket, input_key, input_format, output_bucket,
            output_key, data_access_role_arn)
    
    pprint(start_classification_job)


    print('-' * 88)

    # todo: Job Status

    print("Current job status.")
    job_id = start_classification_job['JobId']
    print(job_id)
    job_desc = comp_detect.describe_job(job_id)

    pprint(job_desc)

    
    print('-' * 88)

if __name__ == '__main__':
  usage_demo()

"""# Step 4: Download the prediction from s3 bucket"""

def aws_session(region_name='us-east-1'):
    return boto3.session.Session(aws_access_key_id='Access Key',
                                aws_secret_access_key='Secret Key',
                                region_name=region_name)

def download_file_from_bucket(bucket_name, s3_key, dst_path):
    session = aws_session()
    s3_resource = session.resource('s3')
    bucket = s3_resource.Bucket(bucket_name)
    bucket.download_file(Key=s3_key, Filename=dst_path)

download_file_from_bucket('lateetuds3east', 'predicted.csv/549102061047-CLN-2ef5a987179e49a05e9d4db12e488a35/output/output.tar.gz', '/content/predicted_download.tar.gz')

"""## Untar the file and save it on local machine"""

# importing the "tarfile" module
import tarfile
  
# open file
file = tarfile.open('predicted_download.tar.gz')
  
# extracting file
file.extractall('/content')
  
file.close()

from google.colab import files

files.download('/content/predictions.jsonl')